---
title: "Auto Insurance Analysis"
author: 'Author : Omar Soub'
date: "Date : 27-05/2024"
output: 
  html_document: 
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    df_print: kable
---



# *Introduction*

-   *On this chapter ,we will use R-Programming Language in order to conduct inferential analysis to analyse the data as much as we can to help decision making*

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(skimr)
library(dplyr)
library(gridExtra)
library(patchwork)
library(effectsize)
library(coin)
library(rstatix)
library(car)
library(dunn.test)
library(nortest)
library(ggstatsplot)
library(FSA)
library(ggplot2)
library(FactoMineR)
library(psych)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
data <- read_csv("C:/Users/Omar/Desktop/Omar_Files/Python_Analysis/Auto_Insurance/Data_Sets/data_car.csv",
               show_col_types=FALSE)
data<- data %>% dplyr::select(-X_OBSTAT_)
```


**------------------------------------------------------------------------------------------------------------------------------------------------**

# *Data Set and Data types view*

------------------------------------------------------------------------

## *Data Set (1st five rows) :*


```{r echo=FALSE, message=FALSE, warning=FALSE}
data %>% head() 
```


------------------------------------------------------------------------

## *data set dimensions*

*lets have a look at the data set dimension :*


```{r echo=FALSE, message=FALSE, warning=FALSE}
data %>% dim() 
```

*The Data set has 10 columns and 67856 rows*

------------------------------------------------------------------------


## *data set variables types*

*lets have a look at the variables types:*

```{r echo=FALSE, message=FALSE, warning=FALSE}
data %>% glimpse() 

```

------------------------------------------------------------------------

*now we will make some data transformation,will do the below :*

*convert agecat ,veh_age,veh_body ,gender and area types into factor*


```{r echo=FALSE, message=FALSE, warning=FALSE}
data$agecat<-as.factor(data$agecat)
data$veh_age<-as.factor(data$veh_age)
data$veh_body<-as.factor(data$veh_body)
data$gender<-as.factor(data$gender)
data$area<-as.factor(data$area)
data$clm<-as.factor(data$clm)
data$numclaims<-as.factor(data$numclaims)
```

------------------------------------------------------------------------

*lets look again to the variables types after datatype conversion :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
data %>% glimpse() 
```


**------------------------------------------------------------------------**

# *Data summary* 

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(data) 
```

------------------------------------------------------------------------


## *Numerical variables EDA *

*we will create new data frame that contain only The needed Numerical variables  : veh_value , exposure  and claimcst0*

```{r echo=FALSE, message=FALSE, warning=FALSE}
Numerical_variables<-data %>% dplyr::select(veh_value , exposure , claimcst0)
```

------------------------------------------------------------------------

### *Numerical_variables summary*


```{r echo=FALSE, message=FALSE, warning=FALSE}
Numerical_variables %>% summarytools::descr() %>% as.data.frame()
```


*Observations:*

* *A. for claims: the mean is 137 ,median is 0 , and the internal quantile range (50% of th data) is 0 as well ==>*

  * *A.1 mean > median and the data is right skewed*
  
  * *A.2 as the data range from (0,inf) ==> the data has gamma distribution*
  
* *B. for exposure: the mean is 0.469 ,median is 0.446 , and the internal quantile range (50% of th data) is 0.999  ==>*
  
  * *B.1 as the data probabilities are not the same. ==> the data has non-uniform distribution* 
  
* *C. for veh_value: the mean is 1777 ,median is 1500 , and the internal quantile range (50% of th data) is 1140 ==>*

  * *C.1 mean > median and the data is right skewed*
  
  * *C.2 as the data range from (0,inf) ==> the data has gamma distribution*  
  

------------------------------------------------------------------------

### *Numerical variables Visualization*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(ggplot2)

(ggplot(data, aes(x = claimcst0)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "claimcst0 histogram distribution",
       x = " ",
       y = " ")+

ggplot(data, aes(x = veh_value)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "veh_value histogram distribution",
       x = " ",
       y = " "))+
  
ggplot(data, aes(x= exposure)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "exposure histogram distribution",
       x = " ",
       y = " ")  
  
```

------------------------------------------------------------------------


### *Correlation between Numerical variables*

*pairs plot*

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
pairs(~.,Numerical_variables)
```


*as the data is not normally distributed , we will see the corr using 'spearman' method*

```{r echo=FALSE, message=FALSE, warning=FALSE}
cor(Numerical_variables,method = "spearman") %>% as.data.frame()
```

------------------------------------------------------------------------

## *Categorical variables EDA*


*we will create new data frame that contain only The needed Categorical variables  :  numclaims, veh_body  veh_age  gender  ,area  and agecat*

```{r echo=FALSE, message=FALSE, warning=FALSE}
Categorical_variables<-data %>% dplyr::select( numclaims, veh_body , veh_age , gender  ,area ,agecat)
```

------------------------------------------------------------------------

### *Categorical_variables summary:*


```{r echo=FALSE, message=FALSE, warning=FALSE}
Categorical_variables %>% summarytools::freq() 
```

*Observations:*

* *A. for numclaims: ==>*

  * *A.1 The number of Risks that have no claims are 63232 (93.19%) out of Total number of Risks 67856*

  * *A.2 The number of Risks that have one claim are 4333 (6.39%) out of Total number of Risks 67856*
  
  * *A.3 The number of Risks that have two claim are 271 (3.99%%) out of Total number of Risks 67856*
  
  * *A.4 The number of Risks that have three claim are 18 (0.27%%) out of Total number of Risks 67856*
  
  * *A.5 The number of Risks that have four claim are 2 (0.0029%%) out of Total number of Risks 67856*
  
  
* *B. for veh_body: ==>*

  * *B.1 SEDAN,HBACK,and STNWG have the most frequencies with 22233(32.76%),18915(27.86%),16261(23.97%) in sequence out of Total number of Risks 67856*

  * *B.2 RDSTR,BUS,and CONVT have the lowest frequencies with 27(0.04%),48(0.07%),81(0.12%) in sequence out of Total number of Risks 67856*
  
  
* *C. for veh_age: ==>*

  * *C.1 The number of Risks with the highest frequency is veh_age==3 with 20064 (29.57%) out of Total number of Risks 67856*

  * *C.2 The number of Risks with the lowest frequency is veh_age==1 with 12257 (18.06%) out of Total number of Risks 67856*
  
  
* *D. for gender: ==>*

  * *D.1 Females has the higher frequency than males with 38603 (56.89%) for females and 29253(43.11%) for males*
  
  
* *F. for veh_age: ==>*

  * *F.1 The number of Risks with the highest frequency is area : C with 20540 (30.27%) out of Total number of Risks 67856*

  * *F.2 The number of Risks with the lowest frequency is area : F  with 3578 (5.27%) out of Total number of Risks 67856*  
  
  
* *G. for agecat: ==>*

  * *G.1 The number of Risks with the highest frequency is agecat == 3 and agecat == 4  C with 16189 (23.86%) ,15767 (23.24%) in sequence out of Total number of Risks 67856*

  * *G.2 The number of Risks with the lowest frequency is agecat == 1 and agecat == 6  with 5742 (8.46%) ,6547 (9.65%) in sequence out of Total number of Risks 67856*    
  

--------------------------------------------------------------------


### *Categorical variables Visualization*

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
(Categorical_variables %>% ggplot(aes(numclaims))+geom_bar(aes(fill=numclaims,color=numclaims),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=5000)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=100)+labs(title = "numclaims Frequency",
       y = " ")+
   
    Categorical_variables %>% ggplot(aes(veh_age))+geom_bar(aes(fill=veh_age,color=veh_age),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=1000)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=50)+labs(title = "veh_age Frequency",
       y = " ")+
   
    Categorical_variables %>% ggplot(aes(agecat))+geom_bar(aes(fill=agecat,color=agecat),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=1000)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=50)+labs(title = "agecat Frequency",
       y = " "))
```


```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
    (Categorical_variables %>% ggplot(aes(gender))+geom_bar(aes(fill=gender,color=gender),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.1,
                                                        nudge_y=100)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=0.1,nudge_y=-300)+labs(title = "gender Frequency",
       y = " ")+coord_flip()+
   
    Categorical_variables %>% ggplot(aes(area))+geom_bar(aes(fill=area,color=area),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.2,
                                                        nudge_y=2000)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=0.2,nudge_y=2000)+labs(title = "area Frequency",
       y = " ")+coord_flip())/
  Categorical_variables %>% ggplot(aes(veh_body))+geom_bar(aes(fill=veh_body,color=veh_body),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=2500)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=70)+labs(title = "veh_body Frequency",
       y = " ")
```


---------------------------------------------------------------------

### *Categorical variables in terms of claims occurrence: *

#### *1. gender :*


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(gtsummary)
data %>% dplyr::select( clm, gender )  %>% 
  gtsummary::tbl_summary(by=clm) %>% 
  add_p() %>% add_overall() %>% add_n() 
```

* *Insights*

*A. Out of 67856 risk , we have 4624 with positive claims claims*  

*B. female had more claim percentage than males*  

*C. the p value of the the pearson chisq test is 0.6 ==> more than 0.05 which statistically means there is no significant difference between genders in respect of claims*

------------------------------------------------------------------------


* *Visualizing gender against clm*

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
data %>% ggbarstats(x=gender,y = clm,label = "both",ggtheme = ggplot2::theme_gray(),sample.size.label.args = list(size = 5),
title ="gender against clm comparison")
```

------------------------------------------------------------------------

*now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=TRUE, message=FALSE, warning=FALSE}
table <- table(data$clm ,data$gender)
chi_square_test<-chisq.test(table)
phi_coefficient <- sqrt(chi_square_test$statistic / sum(table))
cat(paste("The effect size (Phi Coefficient) is",phi_coefficient ,"indicating",interpret_phi(phi_coefficient),"effect size"))
```


* *claims probabilities by gender*

```{r echo=FALSE, message=FALSE, warning=FALSE}
m<-glm(clm~gender*area,data = data,family = binomial())
library(emmeans) 
emmeans::emmeans(m,~gender,type="response",infer=T)
```


* *Insights*

*the probability of females having claims is little higher than the probability of males with .0686 females and 0.0675 for males*


* *To visualize our results :*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(sjPlot)
plot_model(m,type = "eff",terms = c("gender"),title = "Predicted probabilities of claims by gender")
```




------------------------------------------------------------------------


#### *2. area :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(gtsummary)
data %>% dplyr::select( area, clm )  %>% 
  gtsummary::tbl_summary(by=clm) %>% 
  add_p() %>% add_overall() %>% add_n()
```

* *Insights*

*A. area A has the highest number of claims with '31%'  percentage* 

*B. area F has the lowest number  of claims with '6.1%' percentage*  

*C. the p value of the the pearson chisq test is 0.003 ==> less than 0.05 which statistically means there is  significant difference between areas in respect of claims*

------------------------------------------------------------------------


* *Visualizing area against clm *

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
data %>% ggbarstats(x=area,y = clm,label = "both",ggtheme = ggplot2::theme_gray(),sample.size.label.args = list(size = 3),
title ="area against clm comparison",fixed.margin = "cols")+ggplot2::coord_flip()
```

------------------------------------------------------------------------

*now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=TRUE, message=FALSE, warning=FALSE}
library(vcd)
table <- table(data$clm ,data$area)
cat(paste("The effect size (Cramér's V) is ", assocstats(table)$cramer,"indicating",interpret_cramers_v(assocstats(table)$cramer),"effect size"))

```



* *claims probabilities by area*

```{r echo=FALSE, message=FALSE, warning=FALSE}
m<-glm(clm~area,data = data,family = binomial())
library(emmeans) 
emmeans::emmeans(m,~area,type="response",infer=T)
```

* *Insights*

*We can see that area== F has the highest probability of claimed with 0.0783%  and area== D has the lowest probability with 0.0607%*


* *To visualize our results :*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(sjPlot)
plot_model(m,type = "eff",terms = c("area"),title = "Predicted probabilities of claims by area")

```



---------------------------------------------------------------------

#### *3. agecat :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(gtsummary)
data %>% dplyr::select( clm, agecat )  %>% 
  gtsummary::tbl_summary(by=clm) %>% 
  add_p() %>% add_overall() %>% add_n()
```


*Insights*

*A. Age Category 2 and 4 have  the highest number of claims with '24%' percentage for each -'noting that both categories has the highest count of total agecat observations'-*  

*B. Age Category 6  has the lowest number  of claims with '7.9%' percentage*    

*C. the p value of the the pearson chisq test < 0.001 ==> less than 0.05 which statistically means there is  significant difference between Age Categories in respect of claims*

------------------------------------------------------------------------


* *Visualizing Age Category against clm *

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
data %>% ggbarstats(x=agecat,y = clm,label = "both",ggtheme = ggplot2::theme_gray(),sample.size.label.args = list(size = 3),
title ="Age Category against clm comparison",fixed.margin = "cols")+ggplot2::coord_flip()
```

------------------------------------------------------------------------

*now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(vcd)
table <- table(data$clm ,data$agecat)
cat(paste("The effect size (Cramér's V) is ", assocstats(table)$cramer,"indicating",interpret_cramers_v(assocstats(table)$cramer),"effect size"))
```



* *claims probabilities by agecat*

```{r echo=FALSE, message=FALSE, warning=FALSE}
m<-glm(clm~agecat,data = data,family = binomial())
library(emmeans) 
emmeans::emmeans(m,~agecat,type="response",infer=T)
```

* *Insights*

*We can see that agecat== 1 has the highest probability of claimed with 0.0864%  and agecat== 6 has the lowest probability with 0.0558%*


* *To visualize our results :*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(sjPlot)
plot_model(m,type = "eff",terms = c("agecat"),title = "Predicted probabilities of claims by agecat")
```



---------------------------------------------------------------------

#### *4. veh_age :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(gtsummary)
data %>% dplyr::select( clm, veh_age )  %>% 
  gtsummary::tbl_summary(by=clm) %>% 
  add_p() %>% add_overall() %>% add_n()
```



*Insights*

*A. Vehicle Age 3 has highest number of claims with '29%' percentage-'noting that it has the highest count of total Vehicle Age categories observations'-*  

*B. Vehicle Age 1 has the lowest number  of claims with '18%' percentage*   

*C. the p value of the the pearson chisq test < 0.001 ==> less than 0.05 which statistically means there is  significant difference between Vehicle Ages in respect of claims*


------------------------------------------------------------------------


* *Visualizing Vehicle Age Category against clm *

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
data %>% ggbarstats(x=veh_age,y = clm,label = "both",ggtheme = ggplot2::theme_gray(),sample.size.label.args = list(size = 3),
title ="Vehicle Age against clm comparison",fixed.margin = "cols")+ggplot2::coord_flip()
```


------------------------------------------------------------------------

*now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(vcd)
table <- table(data$clm ,data$veh_age)
cat(paste("The effect size (Cramér's V) is ", assocstats(table)$cramer,"indicating",interpret_cramers_v(assocstats(table)$cramer),"effect size"))
```



* *claims probabilities by agecat*

```{r echo=FALSE, message=FALSE, warning=FALSE}
m<-glm(clm~veh_age,data = data,family = binomial())
library(emmeans) 
emmeans::emmeans(m,~veh_age,type="response",infer=T)
```

* *Insights*

*We can see that veh_age== 2 has the highest probability of claimed with 0.0759%  and veh_age== 4 has the lowest probability with 0.0622%*


* *To visualize our results :*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(sjPlot)
plot_model(m,type = "eff",terms = c("veh_age"),title = "Predicted probabilities of claims by veh_age")
```


---------------------------------------------------------------------

#### *5. veh_body :*

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
library(gtsummary)
data %>% dplyr::select( clm, veh_body )  %>% 
  gtsummary::tbl_summary(by=clm) %>% 
  add_p() %>% add_overall() %>% add_n()
```


*Insights*

*A. Sedan veh_body  has highest number of claims with '32%' percentage-'noting that it has the highest count of total veh_body observations'-* 

*B. convt and RDSTR veh_body has the lowest number  of claims with less than '0.1%' percentage*

------------------------------------------------------------------------

* *Visualizing veh_body Category against clm *

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
data %>% ggbarstats(x=veh_body,y = clm,label = "both",ggtheme = ggplot2::theme_gray(),sample.size.label.args = list(size = 3),
title ="veh_body against clm comparison",fixed.margin = "cols")+ggplot2::coord_flip()
```


------------------------------------------------------------------------

*now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(vcd)
table <- table(data$clm ,data$veh_body)
cat(paste("The effect size (Cramér's V) is ", assocstats(table)$cramer,"indicating",interpret_cramers_v(assocstats(table)$cramer),"effect size"))
```



* *claims probabilities by veh_body*

```{r echo=FALSE, message=FALSE, warning=FALSE}
m<-glm(clm~veh_body,data = data,family = binomial())
library(emmeans) 
emmeans::emmeans(m,~veh_body,type="response",infer=T)
```

* *Insights*

*We can see that veh_body== BUS has the highest probability of claimed with 18.75%  and veh_body== CONVT has the lowest probability with 0.0370%*


* *To visualize our results :*

```{r echo=FALSE, fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
library(sjPlot)
plot_model(m,type = "eff",terms = c("veh_body"),title = "Predicted probabilities of claims by veh_body")
```


**------------------------------------------------------------------------**

# *Data summary - Positive claims* 

*positive claims Data is the data with claims > 0*


```{r echo=FALSE, message=FALSE, warning=FALSE}
df<- data %>% dplyr::filter(claimcst0>0) %>% dplyr::select(-clm)

```

------------------------------------------------------------------------


```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(df) 
```


------------------------------------------------------------------------


## *Numerical variables EDA *

*we will create new data frame that contain only The needed Numerical variables  : veh_value , exposure  and claimcst0*

```{r echo=FALSE, message=FALSE, warning=FALSE}
Numerical_variables<-df %>% dplyr::select(veh_value , exposure , claimcst0)
```

------------------------------------------------------------------------

### *Numerical_variables summary*


```{r echo=FALSE, message=FALSE, warning=FALSE}
Numerical_variables %>% summarytools::descr() %>% as.data.frame()
```

*Observations:*

* *A. for claims: the mean is 2014 ,median is 761 , and the internal quantile range (50% of th data) is 1737 as well ==>*

  *-noting that the the mean was 137 ,median was 0 , and the internal quantile range (50% of th data) was 0 as well for all data set-*

  
* *B. for exposure: the mean is 0.611 ,median is 0.638 , and the internal quantile range (50% of th data) is 0.422  ==>*
  
  *-noting that the mean was 0.469 ,median was 0.446 , and the internal quantile range (50% of th data) was 0.999 for all data set-*
  
  
* *C. for veh_value: the mean is 1859 ,median is 1570 , and the internal quantile range (50% of th data) is 1210 ==>*

  *-noting that the mean was 1777 ,median was 1500 , and the internal quantile range (50% of th data) was 1140 for all data set-*
  

------------------------------------------------------------------------

### *Numerical variables Visualization*

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
library(ggplot2)

(ggplot(df, aes(x = claimcst0)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "claimcst0 histogram distribution",
       x = " ",
       y = " ")+

ggplot(df, aes(x = veh_value)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "veh_value histogram distribution",
       x = " ",
       y = " "))+
  
ggplot(df, aes(x= exposure)) +
  geom_histogram(fill = "red", color = "black", linetype = "dashed", alpha = 0.5) +
  labs(title = "exposure histogram distribution",
       x = " ",
       y = " ")  
  
```

------------------------------------------------------------------------

### *Correlation between Numerical variables*

*pairs plot*

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}
pairs(~.,Numerical_variables)
```


*as the data is not normally distributed , we will see the corr using 'spearman' method*

```{r echo=FALSE, message=FALSE, warning=FALSE}
cor(Numerical_variables,method = "spearman") %>% as.data.frame()
```


------------------------------------------------------------------------

## *Categorical variables EDA*


*we will create new data frame that contain only The needed Categorical variables  :  numclaims, veh_body  veh_age  gender  ,area  and agecat*

```{r echo=FALSE, message=FALSE, warning=FALSE}
Categorical_variables<-df %>% dplyr::select( numclaims, veh_body , veh_age , gender  ,area ,agecat)
Categorical_variables %>% head()
```

------------------------------------------------------------------------

### *Categorical_variables summary:*


```{r echo=FALSE, message=FALSE, warning=FALSE}
Categorical_variables %>% summarytools::freq()
```


*Observations:*

* *A. for numclaims: ==>*

  * *A.1 The number of Risks that have one claim are 4333 (93.707) out of Total number of Risks with positive claims 4624*

  * *A.2 The number of Risks that have one claim are 271 (5.861) out of Total number of Risks with positive claims 4624*
  
  * *A.3 The number of Risks that have two claim are 18 (0.389%) out of Total number of Risks with positive claims 4624*
  
  * *A.4 The number of Risks that have three claim are 2 (0.043%) out of Total number of Risks with positive claims 4624*
  
  
  
* *B. for veh_body: ==>*

  * *B.1 SEDAN,HBACK,and STNWG have the most claims incurred frequency  with 1476(31.920%),1264(27.34%),1173(25.37%) in sequence out of Total number of Risks with positive claims 4624*

  * *B.2 RDSTR,CONVT,and CONVT have the lowest claims incurred frequency with 2(0.043%),3(0.056%),9(0.195%) in sequence out of Total number of Risks with positive claims 4624*
  
  
* *C. for veh_age: ==>*

  * *C.1 The number of Risks with the highest claims incurred frequency is veh_age==3 with 1362 (29.46%) out of Total number of Risks with positive claims 4624*

  * *C.2 The number of Risks with the lowest claims incurred frequency is veh_age==1 with 825 (17.84%) out of Total number of Risks with positive claims 4624*
  
  
* *D. for gender: ==>*

  * *D.1 Females has the higher claims incurred frequency than males with 2648 (57.2%) for females and 1976(42.73%) for males*
  
  
* *F. for area: ==>*

  * *F.1 The number of Risks with the highest claims incurred frequency is area : C with 1412 (30.54%) out of Total number of Risks with positive claims 4624*

  * *F.2 The number of Risks with the lowest claims incurred frequency is area : F  with 280 (6.06%) out of Total number of Risks with positive claims 4624*  
  
  
* *G. for agecat: ==>*

  * *G.1 The number of Risks with the highest claims incurred frequency is agecat == 3 and agecat == 4  C with 1113 (27.7%) ,1104 (23.88%) in sequence out of Total number of Risks with positive claims 4624*

  * *G.2 The number of Risks with the lowest claims incurred frequency is agecat == 6  with 365 (7.89%) out of Total number of Risks with positive claims 4624*    
  

--------------------------------------------------------------------


### *Categorical variables Visualization*

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
(Categorical_variables %>% ggplot(aes(numclaims))+geom_bar(aes(fill=numclaims,color=numclaims),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=300)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=100)+labs(title = "numclaims Frequency",
       y = " ")+
   
    Categorical_variables %>% ggplot(aes(veh_age))+geom_bar(aes(fill=veh_age,color=veh_age),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=100)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=50)+labs(title = "veh_age Frequency",
       y = " ")+
   
    Categorical_variables %>% ggplot(aes(agecat))+geom_bar(aes(fill=agecat,color=agecat),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=100)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=50)+labs(title = "agecat Frequency",
       y = " "))
```


```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
    (Categorical_variables %>% ggplot(aes(gender))+geom_bar(aes(fill=gender,color=gender),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=100)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=-300)+labs(title = "gender Frequency",
       y = " ")+coord_flip()+
   
    Categorical_variables %>% ggplot(aes(area))+geom_bar(aes(fill=area,color=area),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=100)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=-150)+labs(title = "area Frequency",
       y = " ")+coord_flip())/
  Categorical_variables %>% ggplot(aes(veh_body))+geom_bar(aes(fill=veh_body,color=veh_body),show.legend = FALSE)+geom_text(aes(label=after_stat(paste(round(count / sum(count) *100),"%"))),stat='count',nudge_x=-0.001,
                                                        nudge_y=170)+ geom_text(aes(label=after_stat(count)),stat='count',nudge_x=-0.001,nudge_y=70)+labs(title = "veh_body Frequency",
       y = " ")
```




**------------------------------------------------------------------------**

# *Inferential Analysis*

## *choosing the Right statistics tests*

* *In Order To determine what types of statistical tests we will apply , we need to to determine Normality of the target variable (claims):*


### *claimcst0 variable Normality check by conducting Visualization*

```{r echo=FALSE, fig.height=5, fig.width=12, message=FALSE, warning=FALSE}
qplot(sample=claimcst0,data=df)+labs(title = "claimcst0 Q-Q plot")
```



------------------------------------------------------------------------

### *claims variable Normality check by Applying Anderson-Darling normality test*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(nortest)
ad.test(df$claimcst0)
```

------------------------------------------------------------------------


*Based on our above analysis results:*

* *Normality: The Anderson-Darling normality test (p-value < 0.05) indicates non-normal distribution.*


*==> Given these results, We will be using non-parametric statistical tests*


------------------------------------------------------------------------

## *Aplying non-parametric statistical tests*

### *A. Mann-Whitney U Test*

*Mann-Whitney U Test -for features that have 2 categories : equivelent to wilcox.test() wilcox rank sum test with continuity correction,better than wilcox sighned rank exct test(that used when variables are paired)*

* *A.1 Mann-Whitney U Test on claimcst0 against gender*

```{r echo=FALSE, message=FALSE, warning=FALSE}
wilcox.test(df$claimcst0~df$gender)
```

*After Applying the test we found that:*

*The pvalue is less than 0.05 --> statistically there is significant difference in the data distribution between genders*

------------------------------------------------------------------------

* *A.2 Visualization*


```{r echo=FALSE, fig.height=4, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=gender,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claims Comparison across genders using wilcox.test",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```


------------------------------------------------------------------------

* *A.3 wilcox_effsize :*

* *now we want to assess and report the effect size and the magnitude of differences between these groups*


```{r echo=FALSE, message=FALSE, warning=FALSE}
effect_size<-wilcox_effsize(formula =claimcst0~gender,data = df,paried=FALSE)
print(effect_size)
```

*The wilcox_effsize test showed that the effect size= .0315 and the indicating small effect size* 

------------------------------------------------------------------------

* *A.4 Assumption Checks:*    

* *Check the homogeneity of variances using Levene's test.*


```{r}
levene_test <- leveneTest(claimcst0~gender,data = df)
print(levene_test)
```

* *The test indicates that the variances are significantly different across the groups (p-value < 0.05).*

------------------------------------------------------------------------

* *A.5 Post-hoc Power Analysis:*

* *Conduct a post-hoc power analysis based on the effect size.*

```{r}
# Post-hoc Power Analysis
#t-test (two samples with unequal n)
library(pwr)
effect_size_value <- effect_size$effsize
power_analysis <- pwr.t2n.test(n1 = length(df$gender=="M"), n2 = length(df$gender=="F"), 
                               d = effect_size_value, 
                               sig.level = 0.05, power = NULL)
print(power_analysis)

```

* *The power of the test is 0.3277674,This means there is a 32.77% chance of detecting a true effect.*



------------------------------------------------------------------------

### *B. kruskal.test*

#### *B.1 kruskal.test on claims against area*


* *B.1.1 kruskal.test on claims against area as it has more than 2 factors*


```{r echo=FALSE, message=FALSE, warning=FALSE}
kruskal.test(df$claimcst0,df$area)
```


*After Applying the test we found that:*

*The pvalue is less than 0.05 --> statistically there is significant difference in the data distribution between areas*

------------------------------------------------------------------------


* *B.1.2 Visualization*


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=area,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claimcst0 Comparison across areas using Kruskal-Wallis",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```

------------------------------------------------------------------------

* *B.1.3 kruskal_effsize :*

* *now we want to assess and report the effect size and the magnitude of differences between these groups*

```{r echo=FALSE, message=FALSE, warning=FALSE}

eff_size<-kruskal_effsize(formula =claimcst0 ~area,data = df)
eff_size
```


* *The kruskal_effsize test showed that the effect size (epsilon_squared)= 0.004680724 and the indicating small effect size*

------------------------------------------------------------------------


* *B.1.4 to find out in details these differences we will Perform Dunn's Test with holm correction for p-values :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(FSA)
dunnTest(claimcst0 ~area,data=df, method="holm")
```

*From the above table we can get that :*

* *The p.adjusted value is less than 0.05  between area B-E ,area A-F,area B-F,area C-F --> statistically there is significant difference in the data distribution between these groups,and there is no statistically significant difference in the data distribution between all other area groups*

------------------------------------------------------------------------


* *B.1.5 Assumption Checks:* 

* *Check the homogeneity of variances using Levene's test.*

```{r}
levene_test <- leveneTest(claimcst0 ~area,data=df)
print(levene_test)
```

* *The test indicates that the variances are significantly different across the groups (p-value < 0.05).*

------------------------------------------------------------------------

* *B.1.6 Post-hoc Power Analysis:*

* *Conduct a post-hoc power analysis based on the effect size.*

```{r}
effect_size <- sqrt(eff_size$effsize)
power_analysis <- pwr.anova.test(k = length(unique(df$area)),
                                 n = nrow(df) / length(unique(df$area)),
                                 f = effect_size,
                                 sig.level = 0.05,
                                 power = NULL)
print(power_analysis)
```

* *The power of the test is 0.966717,This means there is a 96% chance of detecting a true effect.*


------------------------------------------------------------------------

#### *B.2 kruskal.test on claims against veh_body*

* *B.2.1 kruskal.test on claims against veh_body as it has more than 2 factors*


```{r echo=FALSE, message=FALSE, warning=FALSE}
kruskal.test(df$claimcst0,df$veh_body)
```


*After Applying the test we found that:*

*The pvalue is more than 0.05 --> statistically there is no significant difference in the data distribution between veh_body*

------------------------------------------------------------------------

* *B.2.2 Visualization*


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=veh_body,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claimcst0 Comparison across veh_body using Kruskal-Wallis",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```


------------------------------------------------------------------------

#### *B.3 kruskal.test on claims against age category*

* *B.3.1 kruskal.test on claims against age category as it has more than 2 factors*


```{r echo=FALSE, message=FALSE, warning=FALSE}
kruskal.test(df$claimcst0,df$agecat) 
```


*After Applying the test we found that:*

*The pvalue is less than 0.05 --> statistically there is significant difference in the data distribution between agecat*

------------------------------------------------------------------------

* *B.3.2 Visualization*


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=agecat,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claimcst0 Comparison across agecat using Kruskal-Wallis",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```


------------------------------------------------------------------------


* *B.3.3 kruskal_effsize :*

* *now we want to assess and report the effect size and the magnitude of differences between these groups*

```{r echo=FALSE, message=FALSE, warning=FALSE}
eff_size<-kruskal_effsize(formula =claimcst0 ~agecat,data = df)
eff_size
```

* *The kruskal_effsize test showed that the effect size (epsilon_squared)= 0.001320641 and the indicating small effect size*

------------------------------------------------------------------------

* *B.3.4 to find out in details these differences we will Perform Dunn's Test with holm correction for p-values :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(FSA)

dunnTest(df$claimcst0 ~df$agecat,data=df, method="holm")
```

*From the above table we can get that :*

* *The p.adjusted value is less than 0.05 only between agecat 1-5 --> statistically there is significant difference in the data distribution between these groups,and there is no statistically significant difference in the data distribution between all other agecat groups*

------------------------------------------------------------------------

* *B.3.5 Assumption Checks:* 

* *Check the homogeneity of variances using Levene's test.*

```{r}
levene_test <- leveneTest(claimcst0 ~agecat,data=df)
print(levene_test)
```

* *The test indicates that the variances are significantly different across the groups (p-value < 0.05).*

------------------------------------------------------------------------

* *B.3.6 Post-hoc Power Analysis:*

* *Conduct a post-hoc power analysis based on the effect size.*

```{r}
effect_size <- sqrt(eff_size$effsize)
power_analysis <- pwr.anova.test(k = length(unique(df$agecat)),
                                 n = nrow(df) / length(unique(df$agecat)),
                                 f = effect_size,
                                 sig.level = 0.05,
                                 power = NULL)
print(power_analysis)
```

* *The power of the test is 0.4397375,This means there is a 43.9% chance of detecting a true effect.*



------------------------------------------------------------------------

#### *B.4 kruskal.test on claims against number of claims*

* *B.4.1 kruskal.test on claims against number of claims as it has more than 2 factors*


```{r echo=FALSE, message=FALSE, warning=FALSE}
kruskal.test(df$claimcst0,df$numclaims)
```


*After Applying the test we found that:*

*The pvalue is less than 0.05 --> statistically there is significant difference in the data distribution between numclaims*

------------------------------------------------------------------------

* *B.4.2 Visualization*


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=numclaims,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claimcst0 Comparison across numclaims using Kruskal-Wallis",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```

------------------------------------------------------------------------

* *B.4.3 kruskal_effsize :*

* *now we want to assess and report the effect size and the magnitude of differences between these groups*

```{r echo=FALSE, message=FALSE, warning=FALSE}
eff_size<-kruskal_effsize(formula =claimcst0 ~numclaims,data = df)
eff_size
```

* *The kruskal_effsize test showed that the effect size (epsilon_squared)= 0.0337713 and the indicating small effect size*

------------------------------------------------------------------------

* *B.4.4 to find out in details these differences we will Perform Dunn's Test with holm correction for p-values :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(FSA)

dunnTest(df$claimcst0 ~df$numclaims,data=df, method="holm")
```

*From the above table we can get that :*

* *The p.adjusted value is less than 0.05  between numclaims 1-2 ,numclaims 1-3   --> statistically there is significant difference in the data distribution between these groups,and there is no statistically significant differences in the data distribution between all other numclaims groups*

------------------------------------------------------------------------

* *B.4.5 Assumption Checks:* 

* *Check the homogeneity of variances using Levene's test.*

```{r}
levene_test <- leveneTest(claimcst0 ~numclaims,data=df)
print(levene_test)
```

* *The test indicates that the variances are not significantly different across the groups (p-value > 0.05).*

------------------------------------------------------------------------

* *B.4.6 Post-hoc Power Analysis:*

* *Conduct a post-hoc power analysis based on the effect size.*

```{r}
effect_size <- sqrt(eff_size$effsize)
power_analysis <- pwr.anova.test(k = length(unique(df$numclaims)),
                                 n = nrow(df) / length(unique(df$numclaims)),
                                 f = effect_size,
                                 sig.level = 0.05,
                                 power = NULL)
print(power_analysis)
```

* *The power of the test is 1,This means there is a 100% chance of detecting a true effect.*


------------------------------------------------------------------------

#### *B.5 kruskal.test on claims against veh_age*

* *B.5.1 kruskal.test on claims against veh_age as it has more than 2 factors*


```{r echo=FALSE, message=FALSE, warning=FALSE}
kruskal.test(df$claimcst0,df$veh_age)
```


*After Applying the test we found that:*

*The pvalue is less than 0.05 --> statistically there is significant difference in the data distribution between veh_age*

------------------------------------------------------------------------

* *B.5.2 Visualization*


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
df %>% ggbetweenstats(x=veh_age,y=claimcst0,type = "nonparamertic",
                             ggtheme = ggplot2::theme_gray(),title = "claimcst0 Comparison across veh_age using Kruskal-Wallis",
                             ggsignif.args = list(textsize = 3, tip_length = 0.01, na.rm = TRUE))
```

------------------------------------------------------------------------


* *B.5.3 kruskal_effsize :*

* *now we want to assess and report the effect size and the magnitude of differences between these groups*

```{r echo=FALSE, message=FALSE, warning=FALSE}
eff_size<-kruskal_effsize(formula =claimcst0 ~veh_age,data = df)
eff_size
```

* *The kruskal_effsize test showed that the effect size (epsilon_squared)= 0.003391443 and the indicating small effect size*

------------------------------------------------------------------------

* *B.5.4 to find out in details these differences we will Perform Dunn's Test with holm correction for p-values :*

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(FSA)

dunnTest(df$claimcst0 ~df$veh_age,data=df, method="holm")
```

*From the above table we can get that :*

* *The p.adjusted value is less than 0.05 only between veh_age 1-3 ,veh_age 1-4 ,veh_age 2-4   --> statistically there is significant difference in the data distribution between these groups,and there is no statistically significant difference in the data distribution between all other veh_age groups*


------------------------------------------------------------------------

* *B.5.5 Assumption Checks:* 

* *Check the homogeneity of variances using Levene's test.*

```{r}
levene_test <- leveneTest(claimcst0 ~veh_age,data=df)
print(levene_test)
```


* *The test indicates that the variances are not significantly different across the groups (p-value > 0.05).*

------------------------------------------------------------------------

* *B.5.6 Post-hoc Power Analysis:*

* *Conduct a post-hoc power analysis based on the effect size.*

```{r}
effect_size <- sqrt(eff_size$effsize)
power_analysis <- pwr.anova.test(k = length(unique(df$veh_age)),
                                 n = nrow(df) / length(unique(df$veh_age)),
                                 f = effect_size,
                                 sig.level = 0.05,
                                 power = NULL)
print(power_analysis)
```

* *The power of the test is 0.92,This means there is a 92% chance of detecting a true effect.*



**------------------------------------------------------------------------**

# *Insurance risk measurements by factors.*

* *as the insurance risk level is measured by the frequency and the severity; we will calculate the the same and group it by factors.*


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyr)
library(janitor)
library(patchwork)
library(knitr)  # in order to use kable()
library(GGally)
library(broom)
library(MASS)
library(flexplot)
library(insuranceData)
library(insurancerating)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("C:/Users/Omar/Desktop/Omar_Files/Python_Analysis/Auto_Insurance/Data_Sets/data_car.csv",
               show_col_types=FALSE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
data$agecat<-as.factor(data$agecat)
data$veh_age<-as.factor(data$veh_age)
data$veh_body<-as.factor(data$veh_body)
data$gender<-as.factor(data$gender)
data$area<-as.factor(data$area)
```



## *gender factor*

```{r echo=FALSE, message=FALSE, warning=FALSE}
####
univariate(data, x = gender, severity =claimcst0 , nclaims = numclaims,
           exposure = exposure)
```

> *we can see that :*

-   *females number of claims are more than males.*

-   *females claims cost are more than males but; the average severity is less ,and this is due to the larger number of claims(more than males)*

-   *both females and males almost have the same frequency.*

-   *female risk premium is lesser than males ,and this due to the higher exposure females have.*

*the below chart visualize all together for more clear understanding:*


```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
univariate(data, x = gender, severity = claimcst0, nclaims = numclaims,
           exposure = exposure) %>% autoplot(background = FALSE)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
data<-data %>% mutate(across(c(veh_body,veh_age,gender,area,agecat),as.factor)) %>% 
  mutate(across(c(veh_body,veh_age,gender,area,agecat),~biggest_reference(.,exposure))) 

numclaims_glm_gender<-glm(data = data,formula = numclaims~gender,family = poisson,offset = log(exposure))
```



------------------------------------------------------------------------

## *veh_age factor*

```{r echo=FALSE, message=FALSE, warning=FALSE}
####
univariate(data, x = veh_age, severity = claimcst0, nclaims = numclaims,
           exposure = exposure)
```

> *we can see that :*

-   *the claims cost for the vehicle age 1 is the lowest while for vehicle age 3 is the highest.*

-   *vehicle age 1 has the lowest number on claims while the vehicle age 3 has the highest.*

-   *the exposure for the vehicle age 1 is the lowest while for vehicle age 3 is the highest.*

-   *the frequency for the vehicle age 4 is the lowest while for vehicle age 2 is the highest.*

-   *the average_severity for the vehicle age 1 is the lowest while for vehicle age 4 is the highest.*

-   *the risk premium for the vehicle age 3 & 4 is the lowest and almost the same, while its the highest for vehicle age 2*

*the below chart visualize all together for more clear understating :*

```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}

univariate(data, x = veh_age, severity = claimcst0, nclaims = numclaims,
           exposure = exposure) %>% autoplot(background = FALSE)

```



```{r message=FALSE, warning=FALSE, include=FALSE}

numclaims_glm_veh_age<-glm(data = data,formula = numclaims~veh_age,family = poisson,offset = log(exposure))

```


------------------------------------------------------------------------

## *veh_body factor*

```{r echo=FALSE, message=FALSE, warning=FALSE}
####
univariate(data, x = veh_body, severity = claimcst0, nclaims = numclaims,
           exposure = exposure)
```

> *We can see that:*

-   *sedan,Hback,and Stnwg have the highest claims cost (), while RDSTR has the lowest claims cost.*

-   *RDSTR ,convt,Bus,MCARA,COUPE & PANVN have less than 100 claims, SEDAN,STNWG & HBACK have more than 1000 claims ,other have claims between 130 and 276.*

-   *BUS has the highest frequency with 38.3% , (RDSTR,MCARA, and COUPE have frequincies (27.7%,25.3% and 23.5%), other groups have frequencies less than 18%.*

-   *RDSTR has the lowest average severity , (MIBUS,COUPE and TRUCK ) have the highest.*

-   *the risk premium is the less than 200 for RDSTR and MCARA, the more tgan 500 for BUS and COUPE.*

*the below chart visualize all together for more clear understanding:*

```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
####
univariate(data, x = veh_body, severity = claimcst0, nclaims = numclaims,
           exposure = exposure) %>% autoplot(background = FALSE)
```




```{r message=FALSE, warning=FALSE, include=FALSE}
numclaims_glm_veh_body<-glm(data = data,formula = numclaims~veh_body,family = poisson,offset = log(exposure))
```

------------------------------------------------------------------------


## *area factor*

```{r echo=FALSE, message=FALSE, warning=FALSE}
####
univariate(data, x = area, severity = claimcst0, nclaims = numclaims,
           exposure = exposure)
```

> *we can see that :*

-   *the lowest claims cost is for area F , and the highest cost is for area C.*

-   *area F has the lowest number of claims , while area C has the highest .*

-   *area D has the lowest frequency , while area F has the highest.*

-   *area D has the lowest average frequency ,while area F has the highest.*

-   *area D has the lowest risk premium , while area F has the highest.*

*the below chart visualize all together for more clear understanding:*

```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
####
univariate(data, x = area, severity = claimcst0, nclaims = numclaims,
           exposure = exposure) %>% autoplot(background = FALSE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
numclaims_glm_area<-glm(data = data,formula = numclaims~area,family = poisson,offset = log(exposure))
```


------------------------------------------------------------------------

## *age category factor*

```{r echo=FALSE, message=FALSE, warning=FALSE}
univariate(data, x = agecat, severity = claimcst0, nclaims = numclaims,
           exposure = exposure)
```

> *we can see that :*

-   *age category 6 has the lowest claims cost , while age category 4 has the highest*

-   *age category 6 has the lowest number of claims , while age category 4 & 3 has the highest (1185,1189) .*

-   *age category 6 & 5 has the lowest frequency with almost(12.5%) , while age category 1 has the highest with 20%.*

-   *age category 5 has the lowest average frequency ,while age category 1 has the highest.*

-   *age category 5 has the lowest risk premium , while age category 1 has the highest.*

*the below chart visualize all together for more clear understanding:*

```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
univariate(data, x = agecat, severity = claimcst0, nclaims = numclaims,
           exposure = exposure) %>% autoplot(background = FALSE)
```


------------------------------------------------------------------------


```{r message=FALSE, warning=FALSE, include=FALSE}
numclaims_glm_agecat<-glm(data = data,formula = numclaims~agecat,family = poisson,offset = log(exposure))
```


```{r eval=FALSE, include=FALSE}
numclaims_glm_all<-glm(formula = numclaims~veh_body+veh_age+gender+area+agecat,family = poisson,offset = log(exposure),data = data)
numclaims_glm_all %>% summary()
```


```{r eval=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE, include=FALSE}
numclaims_glm_all %>% coefplot::coefplot(title="factors coefficient")

```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
rating_factors(model_data = data,numclaims_glm_all,exposure = exposure)
```



```{r eval=FALSE, fig.height=10, fig.width=10, message=FALSE, warning=FALSE, include=FALSE}
rating_factors(model_data = data,numclaims_glm_all,exposure = exposure) %>% autoplot()
```




```{r eval=FALSE, include=FALSE}
glm(formula = numclaims~veh_body+veh_age+gender+area+agecat,family = poisson,offset = log(exposure),data = data)
```

------------------------------------------------------------------------





